{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv('train.csv')\n",
    "text = df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to parse text punctuation in the set is considered its own word\n",
    "def split_text(sentence):\n",
    "    punctuation = set([\".\",\",\",\"?\",\"!\",\"-\", \":\", \";\"])\n",
    "    r = ''.join([c for c in sentence.lower()])\n",
    "    ls = ''\n",
    "    for c in r:\n",
    "        if c in punctuation:\n",
    "            ls+= ' ' + c + ' '\n",
    "        else:\n",
    "            ls+=c\n",
    "    return ls.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find top 5000 words\n",
    "wordCount = defaultdict(int)\n",
    "avglen = 0\n",
    "for d in text:\n",
    "    for w in split_text(d):\n",
    "        wordCount[w] += 1\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "words = [x[1] for x in counts[:5000]]\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "idToWord = dict(zip(range(len(words)),words))\n",
    "wordSet = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data\n",
    "# removes all words not in the top 5000\n",
    "parsed_text = []\n",
    "for d in text:\n",
    "    split = split_text(d)\n",
    "    parsed_text.append([c for c in split if c in wordSet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in glove model (don't do this if not using glove)\n",
    "def loadGloveModel(gloveFile):\n",
    "    f = open(gloveFile,'r', encoding='utf-8')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "glove = loadGloveModel('glove50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# some constants to set\n",
    "seq_length = 10\n",
    "embedding_length = 50\n",
    "vocab_len = 5000\n",
    "learning_rate = 0.01\n",
    "# this is the loss function for the network (use crossentropy to get useable probabilities)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some helpful torch functions\n",
    "import torch\n",
    "# return the word id (uncomment the 2 lines if you want to use glove representation)\n",
    "def word_tensor(word):\n",
    "    tensor = []\n",
    "    word = word.lower()\n",
    "#     if word in glove:\n",
    "#         tensor = torch.from_numpy(glove[word])\n",
    "    if word in wordId:\n",
    "        tensor = wordId[word]\n",
    "    return tensor\n",
    "# return a tensor representation of a sentence (if using glvoe tensor should be len(sentence),1 ,glove vec length)\n",
    "# and use tensor[li][0] = word_tensor(word) instead\n",
    "def sentence_tensor(sentence):\n",
    "    tensor = torch.zeros(len(sentence),1,1, dtype=torch.long)\n",
    "    for li, word in enumerate(sentence):    \n",
    "        tensor[li] = word_tensor(word)\n",
    "    return tensor\n",
    "# given an output vector return the highest probability word\n",
    "def wordFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    word = top_i[0].item()\n",
    "    return idToWord[word]\n",
    "# formats a training example into a tensor of sequence length glove vectors\n",
    "def parseTrainingExample(i):\n",
    "    target = train_target[i]\n",
    "    sequence = train_sentences[i]\n",
    "    sequence_tensor = sentence_tensor(sequence)\n",
    "    # vector for computing loss\n",
    "    target_tensor = torch.tensor([wordId[target]], dtype=torch.long)\n",
    "    return sequence, target, sequence_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to parse new sentences (make sure no words not in the model are used)\n",
    "def parse_text(sentence):\n",
    "    split = split_text(sentence)\n",
    "    return [c for c in split if c in wordSet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some pre-processing to break the words up into a training matrix with sequence lengths of seq_length\n",
    "train_sentences = []\n",
    "#the next word in the sequence (word to be predicted)\n",
    "train_target = []\n",
    "for t in parsed_text: \n",
    "    for i in range(len(t)-seq_length):\n",
    "        sequence = []\n",
    "        for j in range(seq_length):\n",
    "            sequence.append(t[i+j])\n",
    "        # add the seq_length word sequence to the training examples\n",
    "        train_sentences.append(sequence)\n",
    "        # add the next word after sequence to targets for prediction\n",
    "        train_target.append(t[i+seq_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "import torch.nn.functional as F\n",
    "# if you want to use glove instead remove the embedding layer and just put the input directly into the lstm\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, vocab_size, embedding_dim):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, dropout=.2,num_layers=num_layers)\n",
    "        self.l2o = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embeds = self.word_embeddings(input)\n",
    "        for i in range(seq_length):\n",
    "            lstm_out, hidden = self.lstm(\n",
    "                embeds[i], hidden)\n",
    "        output = self.l2o(lstm_out[0][0])\n",
    "        output = F.log_softmax(output, dim=-1)\n",
    "        return output, hidden\n",
    "    def initHidden(self):\n",
    "        return (torch.zeros(num_layers, 1, self.hidden_size),torch.zeros(num_layers, 1, self.hidden_size))\n",
    "rnn = RNN(embedding_length, embedding_length, vocab_len, num_layers, vocab_len, embedding_length)\n",
    "\n",
    "# to load a pretrained model\n",
    "# rnn.load_state_dict(torch.load('rnn_top5000_first_attempt'))\n",
    "# rnn.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to run one iteration of the network \n",
    "# text_tensor is a tensor representation of the sequence length\n",
    "# hidden is a tensor representation of the hidden state of the lstm\n",
    "# prediction_tensor is a length 1 tensor that conatins the word id of the target word\n",
    "def train(text_tensor, prediction_tensor):\n",
    "    #initialize the hidden state (zero it)\n",
    "    hidden = rnn.initHidden()\n",
    "    # zero the gradient\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    # run the network over the sequence\n",
    "    output, hidden = rnn(text_tensor, hidden)\n",
    "    # compute the loss and calculate auto-grad\n",
    "    loss = criterion(output.view(1,-1), prediction_tensor)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, hidden, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "# some constants for the training step\n",
    "n_epochs = 30\n",
    "n_iters = len(train_sentences)\n",
    "print_every = 500\n",
    "plot_every = 1000\n",
    "\n",
    "# Keep track of losses for plotting\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "# get the time since last print\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(n_epochs):\n",
    "    for iter in range(1,n_iters):\n",
    "\n",
    "        sequence, target, sequence_tensor, target_tensor = parseTrainingExample(iter)\n",
    "        output, hidden, loss = train(sequence_tensor, target_tensor)\n",
    "        current_loss += loss\n",
    "\n",
    "        # Print iter number, loss, name and guess\n",
    "        if iter % print_every == 0:\n",
    "            guess = wordFromOutput(output)\n",
    "            correct = '✓' if guess == target else '✗ (%s)' % target\n",
    "            print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, sequence, guess, correct))\n",
    "\n",
    "        # Add current loss avg to list of losses\n",
    "        if iter % plot_every == 0:\n",
    "            all_losses.append(current_loss / plot_every)\n",
    "            current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will save the model, use a different file name for different models\n",
    "torch.save(rnn.state_dict(), 'filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the avg loss over training time \n",
    "# this just shows if training acomplished anything\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Just return a vector of probabilities of the next word given a sequence\n",
    "def evaluate(sequence_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "    output = []\n",
    "    output, hidden = rnn(sequence_tensor, hidden)\n",
    "    return output\n",
    "\n",
    "# this is the seed sentence should be of length embedding_len\n",
    "input_sentence = ['The', 'details', 'were', 'examined', 'and', 'it', 'was', 'found', 'that', 'the']\n",
    "vec = []\n",
    "#this will generate 100 different sentences of length 10 + size of input sentence\n",
    "for j in range(1):\n",
    "    out = [d for d in input_sentence]\n",
    "    for i in range(10):\n",
    "        # get the probabilities\n",
    "        with torch.no_grad():\n",
    "            output = evaluate(sentence_tensor(out[i:i+seq_length]))\n",
    "        # the probabilities are log probs so use the exponential function\n",
    "        z = []\n",
    "        for d in output:\n",
    "            z.append(math.exp(d.item()))\n",
    "        # due to floating point error the sum is usually 1 + epsilon for some tiny epsilon\n",
    "        # the random choice function requires sum = 1 so we are gonna normalize the l1 norm of the vector to be 1\n",
    "        sum = (np.sum(z))\n",
    "        for index,item in enumerate(z):\n",
    "            z[index] = item/sum\n",
    "        # randomly sample the distribution and append the selected word\n",
    "        choice = (np.random.choice(range(5000), p=z))\n",
    "        out.append(idToWord[choice])\n",
    "    vec.append(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the text genreated\n",
    "df = pd.DataFrame(vec)\n",
    "df.to_csv(\"100_tests_3.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
